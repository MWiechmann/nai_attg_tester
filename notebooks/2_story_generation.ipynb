{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 candidates from ../data/genre_clio_candidates.csv\n",
      "Generating story 1/30 for candidate: Slice of Life\n",
      "Initial max_context: 8192\n",
      "Initial prompt length: 6 words\n",
      "Current prompt tokens: 8\n",
      "Remaining words: 8000\n",
      "Available tokens: 8184\n",
      "Max length for generation: 8183\n",
      "Generated text length: 101 words\n",
      "Full story length: 101 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 150\n",
      "Remaining words: 7899\n",
      "Available tokens: 8042\n",
      "Max length for generation: 8041\n",
      "Generated text length: 114 words\n",
      "Full story length: 215 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 300\n",
      "Remaining words: 7785\n",
      "Available tokens: 7892\n",
      "Max length for generation: 7891\n",
      "Generated text length: 127 words\n",
      "Full story length: 342 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 450\n",
      "Remaining words: 7658\n",
      "Available tokens: 7742\n",
      "Max length for generation: 7741\n",
      "Generated text length: 123 words\n",
      "Full story length: 464 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 600\n",
      "Remaining words: 7536\n",
      "Available tokens: 7592\n",
      "Max length for generation: 7591\n",
      "Generated text length: 110 words\n",
      "Full story length: 574 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 750\n",
      "Remaining words: 7426\n",
      "Available tokens: 7442\n",
      "Max length for generation: 7441\n",
      "Generated text length: 118 words\n",
      "Full story length: 692 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 900\n",
      "Remaining words: 7308\n",
      "Available tokens: 7292\n",
      "Max length for generation: 7291\n",
      "Generated text length: 128 words\n",
      "Full story length: 820 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1050\n",
      "Remaining words: 7180\n",
      "Available tokens: 7142\n",
      "Max length for generation: 7141\n",
      "Generated text length: 116 words\n",
      "Full story length: 936 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1200\n",
      "Remaining words: 7064\n",
      "Available tokens: 6992\n",
      "Max length for generation: 6991\n",
      "Generated text length: 123 words\n",
      "Full story length: 1059 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1350\n",
      "Remaining words: 6941\n",
      "Available tokens: 6842\n",
      "Max length for generation: 6841\n",
      "Generated text length: 123 words\n",
      "Full story length: 1182 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1500\n",
      "Remaining words: 6818\n",
      "Available tokens: 6692\n",
      "Max length for generation: 6691\n",
      "Generated text length: 122 words\n",
      "Full story length: 1304 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1650\n",
      "Remaining words: 6696\n",
      "Available tokens: 6542\n",
      "Max length for generation: 6541\n",
      "Generated text length: 124 words\n",
      "Full story length: 1428 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1800\n",
      "Remaining words: 6572\n",
      "Available tokens: 6392\n",
      "Max length for generation: 6391\n",
      "Generated text length: 114 words\n",
      "Full story length: 1542 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1950\n",
      "Remaining words: 6458\n",
      "Available tokens: 6242\n",
      "Max length for generation: 6241\n",
      "Generated text length: 114 words\n",
      "Full story length: 1656 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 2100\n",
      "Remaining words: 6344\n",
      "Available tokens: 6092\n",
      "Max length for generation: 6091\n",
      "Generated text length: 120 words\n",
      "Full story length: 1776 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 2250\n",
      "Remaining words: 6224\n",
      "Available tokens: 5942\n",
      "Max length for generation: 5941\n",
      "Generated text length: 108 words\n",
      "Full story length: 1884 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 2400\n",
      "Remaining words: 6116\n",
      "Available tokens: 5792\n",
      "Max length for generation: 5791\n",
      "Generated text length: 123 words\n",
      "Full story length: 2006 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 2550\n",
      "Remaining words: 5994\n",
      "Available tokens: 5642\n",
      "Max length for generation: 5641\n",
      "Generated text length: 121 words\n",
      "Full story length: 2127 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 2700\n",
      "Remaining words: 5873\n",
      "Available tokens: 5492\n",
      "Max length for generation: 5491\n",
      "Generated text length: 116 words\n",
      "Full story length: 2243 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 2850\n",
      "Remaining words: 5757\n",
      "Available tokens: 5342\n",
      "Max length for generation: 5341\n",
      "Generated text length: 107 words\n",
      "Full story length: 2350 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 3000\n",
      "Remaining words: 5650\n",
      "Available tokens: 5192\n",
      "Max length for generation: 5191\n",
      "Generated text length: 110 words\n",
      "Full story length: 2460 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 3150\n",
      "Remaining words: 5540\n",
      "Available tokens: 5042\n",
      "Max length for generation: 5041\n",
      "Generated text length: 120 words\n",
      "Full story length: 2580 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 3300\n",
      "Remaining words: 5420\n",
      "Available tokens: 4892\n",
      "Max length for generation: 4891\n",
      "Generated text length: 112 words\n",
      "Full story length: 2691 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 3450\n",
      "Remaining words: 5309\n",
      "Available tokens: 4742\n",
      "Max length for generation: 4741\n",
      "Generated text length: 121 words\n",
      "Full story length: 2812 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 3600\n",
      "Remaining words: 5188\n",
      "Available tokens: 4592\n",
      "Max length for generation: 4591\n",
      "Generated text length: 106 words\n",
      "Full story length: 2917 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 3750\n",
      "Remaining words: 5083\n",
      "Available tokens: 4442\n",
      "Max length for generation: 4441\n",
      "Generated text length: 123 words\n",
      "Full story length: 3040 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 3900\n",
      "Remaining words: 4960\n",
      "Available tokens: 4292\n",
      "Max length for generation: 4291\n",
      "Generated text length: 110 words\n",
      "Full story length: 3150 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 4050\n",
      "Remaining words: 4850\n",
      "Available tokens: 4142\n",
      "Max length for generation: 4141\n",
      "Generated text length: 124 words\n",
      "Full story length: 3274 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 4200\n",
      "Remaining words: 4726\n",
      "Available tokens: 3992\n",
      "Max length for generation: 3991\n",
      "Generated text length: 129 words\n",
      "Full story length: 3403 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 4350\n",
      "Remaining words: 4597\n",
      "Available tokens: 3842\n",
      "Max length for generation: 3841\n",
      "Generated text length: 124 words\n",
      "Full story length: 3527 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 4500\n",
      "Remaining words: 4473\n",
      "Available tokens: 3692\n",
      "Max length for generation: 3691\n",
      "Generated text length: 127 words\n",
      "Full story length: 3653 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 4650\n",
      "Remaining words: 4347\n",
      "Available tokens: 3542\n",
      "Max length for generation: 3541\n",
      "Generated text length: 106 words\n",
      "Full story length: 3759 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 4800\n",
      "Remaining words: 4241\n",
      "Available tokens: 3392\n",
      "Max length for generation: 3391\n",
      "Generated text length: 108 words\n",
      "Full story length: 3867 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 4950\n",
      "Remaining words: 4133\n",
      "Available tokens: 3242\n",
      "Max length for generation: 3241\n",
      "Generated text length: 111 words\n",
      "Full story length: 3977 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 5100\n",
      "Remaining words: 4023\n",
      "Available tokens: 3092\n",
      "Max length for generation: 3091\n",
      "Generated text length: 108 words\n",
      "Full story length: 4085 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 5250\n",
      "Remaining words: 3915\n",
      "Available tokens: 2942\n",
      "Max length for generation: 2941\n",
      "Generated text length: 116 words\n",
      "Full story length: 4201 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 5400\n",
      "Remaining words: 3799\n",
      "Available tokens: 2792\n",
      "Max length for generation: 2791\n",
      "Generated text length: 119 words\n",
      "Full story length: 4320 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 5550\n",
      "Remaining words: 3680\n",
      "Available tokens: 2642\n",
      "Max length for generation: 2641\n",
      "Generated text length: 125 words\n",
      "Full story length: 4445 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 5700\n",
      "Remaining words: 3555\n",
      "Available tokens: 2492\n",
      "Max length for generation: 2491\n",
      "Generated text length: 112 words\n",
      "Full story length: 4557 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 5850\n",
      "Remaining words: 3443\n",
      "Available tokens: 2342\n",
      "Max length for generation: 2341\n",
      "Generated text length: 117 words\n",
      "Full story length: 4674 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 6000\n",
      "Remaining words: 3326\n",
      "Available tokens: 2192\n",
      "Max length for generation: 2191\n",
      "Generated text length: 117 words\n",
      "Full story length: 4791 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 6150\n",
      "Remaining words: 3209\n",
      "Available tokens: 2042\n",
      "Max length for generation: 2041\n",
      "Generated text length: 118 words\n",
      "Full story length: 4909 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 6300\n",
      "Remaining words: 3091\n",
      "Available tokens: 1892\n",
      "Max length for generation: 1891\n",
      "Generated text length: 113 words\n",
      "Full story length: 5022 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 6450\n",
      "Remaining words: 2978\n",
      "Available tokens: 1742\n",
      "Max length for generation: 1741\n",
      "Generated text length: 106 words\n",
      "Full story length: 5128 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 6600\n",
      "Remaining words: 2872\n",
      "Available tokens: 1592\n",
      "Max length for generation: 1591\n",
      "Generated text length: 113 words\n",
      "Full story length: 5240 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 6750\n",
      "Remaining words: 2760\n",
      "Available tokens: 1442\n",
      "Max length for generation: 1441\n",
      "Generated text length: 117 words\n",
      "Full story length: 5357 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 6900\n",
      "Remaining words: 2643\n",
      "Available tokens: 1292\n",
      "Max length for generation: 1291\n",
      "Generated text length: 117 words\n",
      "Full story length: 5474 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 7050\n",
      "Remaining words: 2526\n",
      "Available tokens: 1142\n",
      "Max length for generation: 1141\n",
      "Generated text length: 115 words\n",
      "Full story length: 5589 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 7200\n",
      "Remaining words: 2411\n",
      "Available tokens: 992\n",
      "Max length for generation: 991\n",
      "Generated text length: 121 words\n",
      "Full story length: 5710 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 7350\n",
      "Remaining words: 2290\n",
      "Available tokens: 842\n",
      "Max length for generation: 841\n",
      "Generated text length: 116 words\n",
      "Full story length: 5826 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 7500\n",
      "Remaining words: 2174\n",
      "Available tokens: 692\n",
      "Max length for generation: 691\n",
      "Generated text length: 116 words\n",
      "Full story length: 5942 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 7650\n",
      "Remaining words: 2058\n",
      "Available tokens: 542\n",
      "Max length for generation: 541\n",
      "Generated text length: 125 words\n",
      "Full story length: 6067 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 7800\n",
      "Remaining words: 1933\n",
      "Available tokens: 392\n",
      "Max length for generation: 391\n",
      "Generated text length: 124 words\n",
      "Full story length: 6191 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 7950\n",
      "Remaining words: 1809\n",
      "Available tokens: 242\n",
      "Max length for generation: 241\n",
      "Generated text length: 122 words\n",
      "Full story length: 6312 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 8100\n",
      "Remaining words: 1688\n",
      "Available tokens: 92\n",
      "Max length for generation: 91\n",
      "Generated text length: 79 words\n",
      "Full story length: 6391 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 8191\n",
      "Remaining words: 1609\n",
      "Available tokens: 1\n",
      "Max length for generation: 1\n",
      "Generated text length: 1 words\n",
      "Full story length: 6391 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 1609\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 1609\n",
      "Available tokens: 1638\n",
      "Max length for generation: 1637\n",
      "Generated text length: 118 words\n",
      "Full story length: 6509 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 1491\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 1491\n",
      "Available tokens: 1638\n",
      "Max length for generation: 1637\n",
      "Generated text length: 126 words\n",
      "Full story length: 6635 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 1365\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 1365\n",
      "Available tokens: 1638\n",
      "Max length for generation: 1637\n",
      "Generated text length: 113 words\n",
      "Full story length: 6748 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 1252\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 1252\n",
      "Available tokens: 1638\n",
      "Max length for generation: 1637\n",
      "Generated text length: 122 words\n",
      "Full story length: 6870 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 1130\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 1130\n",
      "Available tokens: 1638\n",
      "Max length for generation: 1502\n",
      "Generated text length: 125 words\n",
      "Full story length: 6995 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 1005\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 1005\n",
      "Available tokens: 1638\n",
      "Max length for generation: 1336\n",
      "Generated text length: 121 words\n",
      "Full story length: 7116 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 884\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 884\n",
      "Available tokens: 1638\n",
      "Max length for generation: 1175\n",
      "Generated text length: 110 words\n",
      "Full story length: 7225 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 775\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 775\n",
      "Available tokens: 1638\n",
      "Max length for generation: 1030\n",
      "Generated text length: 127 words\n",
      "Full story length: 7352 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 648\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 648\n",
      "Available tokens: 1638\n",
      "Max length for generation: 861\n",
      "Generated text length: 125 words\n",
      "Full story length: 7477 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 523\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 523\n",
      "Available tokens: 1638\n",
      "Max length for generation: 695\n",
      "Generated text length: 119 words\n",
      "Full story length: 7596 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 404\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 404\n",
      "Available tokens: 1638\n",
      "Max length for generation: 537\n",
      "Generated text length: 120 words\n",
      "Full story length: 7716 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 284\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 284\n",
      "Available tokens: 1638\n",
      "Max length for generation: 377\n",
      "Generated text length: 126 words\n",
      "Full story length: 7841 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 159\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 159\n",
      "Available tokens: 1638\n",
      "Max length for generation: 211\n",
      "Generated text length: 122 words\n",
      "Full story length: 7962 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Current prompt tokens: 8192\n",
      "Remaining words: 38\n",
      "Available tokens: 0\n",
      "Max length for generation: 1\n",
      "Adjusted max_length to -1 to prevent exceeding max_context\n",
      "Cannot generate more tokens without exceeding max_context. Trimming context.\n",
      "Current prompt tokens: 6554\n",
      "Remaining words: 38\n",
      "Available tokens: 1638\n",
      "Max length for generation: 50\n",
      "Generated text length: 41 words\n",
      "Full story length: 8003 words\n",
      "Trimmed prompt to 8192 tokens\n",
      "Saved story for candidate: Slice of Life to ../data/genre_clio_stories.csv\n",
      "Generating story 2/30 for candidate: Slice of Life\n",
      "Initial max_context: 8192\n",
      "Initial prompt length: 6 words\n",
      "Current prompt tokens: 8\n",
      "Remaining words: 8000\n",
      "Available tokens: 8184\n",
      "Max length for generation: 8183\n",
      "Generated text length: 89 words\n",
      "Full story length: 89 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 150\n",
      "Remaining words: 7911\n",
      "Available tokens: 8042\n",
      "Max length for generation: 8041\n",
      "Generated text length: 96 words\n",
      "Full story length: 184 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 300\n",
      "Remaining words: 7816\n",
      "Available tokens: 7892\n",
      "Max length for generation: 7891\n",
      "Generated text length: 87 words\n",
      "Full story length: 271 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 450\n",
      "Remaining words: 7729\n",
      "Available tokens: 7742\n",
      "Max length for generation: 7741\n",
      "Generated text length: 111 words\n",
      "Full story length: 381 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 600\n",
      "Remaining words: 7619\n",
      "Available tokens: 7592\n",
      "Max length for generation: 7591\n",
      "Generated text length: 130 words\n",
      "Full story length: 510 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 750\n",
      "Remaining words: 7490\n",
      "Available tokens: 7442\n",
      "Max length for generation: 7441\n",
      "Generated text length: 108 words\n",
      "Full story length: 618 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 900\n",
      "Remaining words: 7382\n",
      "Available tokens: 7292\n",
      "Max length for generation: 7291\n",
      "Generated text length: 114 words\n",
      "Full story length: 732 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1050\n",
      "Remaining words: 7268\n",
      "Available tokens: 7142\n",
      "Max length for generation: 7141\n",
      "Generated text length: 80 words\n",
      "Full story length: 812 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1200\n",
      "Remaining words: 7188\n",
      "Available tokens: 6992\n",
      "Max length for generation: 6991\n",
      "Generated text length: 82 words\n",
      "Full story length: 894 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1350\n",
      "Remaining words: 7106\n",
      "Available tokens: 6842\n",
      "Max length for generation: 6841\n",
      "Generated text length: 86 words\n",
      "Full story length: 980 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1500\n",
      "Remaining words: 7020\n",
      "Available tokens: 6692\n",
      "Max length for generation: 6691\n",
      "Generated text length: 109 words\n",
      "Full story length: 1089 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1650\n",
      "Remaining words: 6911\n",
      "Available tokens: 6542\n",
      "Max length for generation: 6541\n",
      "Generated text length: 77 words\n",
      "Full story length: 1165 words\n",
      "Using full story as prompt\n",
      "Current prompt tokens: 1800\n",
      "Remaining words: 6835\n",
      "Available tokens: 6392\n",
      "Max length for generation: 6391\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 291\u001b[0m\n\u001b[0;32m    288\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStory generation complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\nai-attg-tester-CNuytz7O-py3.11\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\nai-attg-tester-CNuytz7O-py3.11\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\nai-attg-tester-CNuytz7O-py3.11\\Lib\\site-packages\\nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     heappop(scheduled)\n\u001b[0;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\selectors.py:323\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    321\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\selectors.py:314\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 314\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m select\u001b[38;5;241m.\u001b[39mselect(r, w, w, timeout)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Story Generation\n",
    "# Imports\n",
    "\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from novelai_api.NovelAI_API import NovelAIAPI\n",
    "from novelai_api.Preset import Model, Preset\n",
    "from novelai_api.GlobalSettings import GlobalSettings\n",
    "from novelai_api.Tokenizer import Tokenizer\n",
    "from novelai_api.utils import b64_to_tokens, tokens_to_b64\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def parse_config_value(value):\n",
    "    \"\"\"Parse a config value, preserving spaces if it's a quoted string.\"\"\"\n",
    "    value = value.strip()\n",
    "    if (value.startswith(\"'\") and value.endswith(\"'\")) or \\\n",
    "       (value.startswith('\"') and value.endswith('\"')):\n",
    "        return ast.literal_eval(value)\n",
    "    return value\n",
    "\n",
    "# Read Settings\n",
    "config_file = '../config/genre_clio_settings.ini'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "# Access the General Settings\n",
    "run_name = config['GENERAL']['run_name']\n",
    "auth_method = config['GENERAL']['auth_method']\n",
    "\n",
    "# Access Story Generation Settings\n",
    "delay_time = int(config['STORY GENERATION - GEN SETTINGS']['delay_time'])\n",
    "generation_timeout = int(config['STORY GENERATION - GEN SETTINGS']['generation_timeout'])\n",
    "max_failed_gens = int(config['STORY GENERATION - GEN SETTINGS']['max_failed_gens'])\n",
    "stories_per_candidate_goal = int(config['STORY GENERATION - GEN SETTINGS']['stories_per_candidate_goal'])\n",
    "story_words = int(config['STORY GENERATION - GEN SETTINGS']['story_words'])\n",
    "bias_phrases = ast.literal_eval(config['STORY GENERATION - GEN SETTINGS']['bias_phrases'])\n",
    "model_class, model_attr = config['STORY GENERATION - GEN SETTINGS']['model'].split('.')\n",
    "model = getattr(globals()[model_class], model_attr)\n",
    "max_context = int(config['STORY GENERATION - GEN SETTINGS']['max_context'])\n",
    "prompt_prefix = parse_config_value(config['STORY GENERATION - GEN SETTINGS']['prompt_prefix'])\n",
    "prompt_suffix = parse_config_value(config['STORY GENERATION - GEN SETTINGS']['prompt_suffix'])\n",
    "\n",
    "# Access the Preset Configuration\n",
    "preset_method = config['STORY GENERATION - PRESET']['preset_method']\n",
    "preset_name = config['STORY GENERATION - PRESET']['preset_name']\n",
    "\n",
    "if preset_method == \"custom\":\n",
    "    preset_stop_sequences = ast.literal_eval(config['STORY GENERATION - PRESET']['preset_stop_sequences'])\n",
    "    preset_temperature = float(config['STORY GENERATION - PRESET']['preset_temperature'])\n",
    "    preset_max_length = int(config['STORY GENERATION - PRESET']['preset_max_length'])\n",
    "    preset_min_length = int(config['STORY GENERATION - PRESET']['preset_min_length'])\n",
    "    preset_top_k = int(config['STORY GENERATION - PRESET']['preset_top_k'])\n",
    "    preset_top_a = float(config['STORY GENERATION - PRESET']['preset_top_a'])\n",
    "    preset_top_p = float(config['STORY GENERATION - PRESET']['preset_top_p'])\n",
    "    preset_typical_p = float(config['STORY GENERATION - PRESET']['preset_typical_p'])\n",
    "    preset_tail_free_sampling = float(config['STORY GENERATION - PRESET']['preset_tail_free_sampling'])\n",
    "    preset_repetition_penalty = float(config['STORY GENERATION - PRESET']['preset_repetition_penalty'])\n",
    "    preset_repetition_penalty_range = int(config['STORY GENERATION - PRESET']['preset_repetition_penalty_range'])\n",
    "    preset_repetition_penalty_slope = float(config['STORY GENERATION - PRESET']['preset_repetition_penalty_slope'])\n",
    "    preset_repetition_penalty_frequency = float(config['STORY GENERATION - PRESET']['preset_repetition_penalty_frequency'])\n",
    "    preset_repetition_penalty_presence = float(config['STORY GENERATION - PRESET']['preset_repetition_penalty_presence'])\n",
    "    preset_repetition_penalty_whitelist = ast.literal_eval(config['STORY GENERATION - PRESET']['preset_repetition_penalty_whitelist'])\n",
    "    preset_repetition_penalty_default_whitelist = config['STORY GENERATION - PRESET']['preset_repetition_penalty_default_whitelist'] == 'True'\n",
    "    preset_length_penalty = float(config['STORY GENERATION - PRESET']['preset_length_penalty'])\n",
    "    preset_diversity_penalty = float(config['STORY GENERATION - PRESET']['preset_diversity_penalty'])\n",
    "    preset_order = ast.literal_eval(config['STORY GENERATION - PRESET']['preset_order'])\n",
    "    preset_phrase_rep_pen = config['STORY GENERATION - PRESET']['preset_phrase_rep_pen']\n",
    "\n",
    "    preset = Preset(name=preset_name, model=model, settings={\n",
    "        'temperature': preset_temperature,\n",
    "        'max_length': preset_max_length,\n",
    "        'min_length': preset_min_length,\n",
    "        'top_k': preset_top_k,\n",
    "        'top_a': preset_top_a,\n",
    "        'top_p': preset_top_p,\n",
    "        'typical_p': preset_typical_p,\n",
    "        'tail_free_sampling': preset_tail_free_sampling,\n",
    "        'repetition_penalty': preset_repetition_penalty,\n",
    "        'repetition_penalty_range': preset_repetition_penalty_range,\n",
    "        'repetition_penalty_slope': preset_repetition_penalty_slope,\n",
    "        'repetition_penalty_frequency': preset_repetition_penalty_frequency,\n",
    "        'repetition_penalty_presence': preset_repetition_penalty_presence,\n",
    "        'repetition_penalty_whitelist': preset_repetition_penalty_whitelist,\n",
    "        'repetition_penalty_default_whitelist': preset_repetition_penalty_default_whitelist,\n",
    "        'length_penalty': preset_length_penalty,\n",
    "        'diversity_penalty': preset_diversity_penalty,\n",
    "        'order': preset_order,\n",
    "        'phrase_rep_pen': preset_phrase_rep_pen,\n",
    "    })\n",
    "\n",
    "elif preset_method == \"official\":\n",
    "    preset = preset_name  # We'll use this string to get the official preset in gen_story\n",
    "else:\n",
    "    raise ValueError(f\"Invalid preset_method: {preset_method}. Must be 'custom' or 'official'.\")\n",
    "\n",
    "auth = False\n",
    "env = os.environ\n",
    "\n",
    "# Init variable for login method\n",
    "if auth_method == \"enter_key\":\n",
    "    auth = input(\"Enter your NovelAI access key: \")\n",
    "if auth_method == \"enter_token\":\n",
    "    auth = input(\"Enter your NovelAI access token: \")\n",
    "elif auth_method == \"enter_login\":\n",
    "    auth = {}\n",
    "    auth[\"user\"] = input(\"Enter your NovelAI username: \")\n",
    "    auth[\"pw\"] = input(\"Enter your NovelAI password: \")\n",
    "elif auth_method == \"env_key\":\n",
    "    auth = env[\"NAI_KEY\"]\n",
    "elif auth_method == \"env_token\":\n",
    "    auth = env[\"NAI_TOKEN\"]\n",
    "elif auth_method == \"env_login\":\n",
    "    auth = {}\n",
    "    auth[\"user\"] = env[\"NAI_USERNAME\"]\n",
    "    auth[\"pw\"] = env[\"NAI_PASSWORD\"]\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"Invalid value for 'auth_method'. Must be one of 'enter_key', 'enter_token', 'enter_login', 'env_key', 'env_token' or 'env_login\"\n",
    "    )\n",
    "# Define necessary functions\n",
    "\n",
    "async def nai_login(api, auth_method, auth):\n",
    "    if auth_method == \"enter_key\" or auth_method == \"env_key\":\n",
    "        await api.high_level.login_from_key(auth)\n",
    "    elif auth_method == \"enter_token\" or auth_method == \"env_token\":\n",
    "        await api.high_level.login_with_token(auth)\n",
    "    elif auth_method == \"enter_login\" or auth_method == \"env_login\":\n",
    "        await api.high_level.login(auth[\"user\"], auth[\"pw\"])\n",
    "\n",
    "async def gen_story(api, prompt, model, preset, max_length=2048):\n",
    "    global_settings = GlobalSettings()\n",
    "    if isinstance(preset, str):\n",
    "        preset = Preset.from_official(model, preset)\n",
    "    preset['max_length'] = min(max(max_length, 1), 2048)  # Ensure max_length is between 1 and 2048\n",
    "    gen = await api.high_level.generate(\n",
    "        prompt, model, preset, global_settings, None, None, None\n",
    "    )\n",
    "    generated_text = Tokenizer.decode(model, b64_to_tokens(gen[\"output\"]))\n",
    "    return generated_text\n",
    "\n",
    "async def generate_full_story(api, prompt, model, preset, story_words, max_retries=3, verbose=False):\n",
    "    full_story = \"\"\n",
    "    current_prompt = prompt\n",
    "    if verbose:\n",
    "        print(f\"Initial max_context: {max_context}\")\n",
    "        print(f\"Initial prompt length: {len(prompt.split())} words\")\n",
    "    \n",
    "    while len(full_story.split()) < story_words:\n",
    "        remaining_words = story_words - len(full_story.split())\n",
    "        prompt_tokens = Tokenizer.encode(model, current_prompt)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Current prompt tokens: {len(prompt_tokens)}\")\n",
    "            print(f\"Remaining words: {remaining_words}\")\n",
    "        \n",
    "        available_tokens = max_context - len(prompt_tokens)\n",
    "        max_length = max(1, min(available_tokens - 1, int(remaining_words * 1.33)))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Available tokens: {available_tokens}\")\n",
    "            print(f\"Max length for generation: {max_length}\")\n",
    "        \n",
    "        if len(prompt_tokens) + max_length > max_context:\n",
    "            max_length = max_context - len(prompt_tokens) - 1\n",
    "            if verbose:\n",
    "                print(f\"Adjusted max_length to {max_length} to prevent exceeding max_context\")\n",
    "        \n",
    "        if max_length < 1:\n",
    "            print(\"Cannot generate more tokens without exceeding max_context. Trimming context.\")\n",
    "            # Trim the current prompt to make room for new generation\n",
    "            trim_tokens = int(max_context * 0.2)  # Trim 20% of max_context\n",
    "            current_prompt_tokens = prompt_tokens[trim_tokens:]\n",
    "            current_prompt = Tokenizer.decode(model, current_prompt_tokens)\n",
    "            continue\n",
    "        \n",
    "        for _ in range(max_retries):\n",
    "            try:\n",
    "                generated_text = await gen_story(api, current_prompt, model, preset, max_length)\n",
    "                full_story += generated_text\n",
    "                if verbose:\n",
    "                    print(f\"Generated text length: {len(generated_text.split())} words\")\n",
    "                    print(f\"Full story length: {len(full_story.split())} words\")\n",
    "                \n",
    "                # Update the prompt for the next iteration\n",
    "                full_story_tokens = Tokenizer.encode(model, full_story)\n",
    "                if len(full_story_tokens) > max_context:\n",
    "                    # If the full story exceeds max_context, trim it\n",
    "                    trimmed_tokens = full_story_tokens[-max_context:]\n",
    "                    current_prompt = Tokenizer.decode(model, trimmed_tokens)\n",
    "                    if verbose:\n",
    "                        print(f\"Trimmed prompt to {len(trimmed_tokens)} tokens\")\n",
    "                else:\n",
    "                    current_prompt = full_story\n",
    "                    if verbose:\n",
    "                        print(\"Using full story as prompt\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {e}. Retrying...\")\n",
    "        else:\n",
    "            print(f\"Failed to generate after {max_retries} attempts. Stopping generation.\")\n",
    "            break\n",
    "    \n",
    "    return full_story\n",
    "\n",
    "def save_story_to_csv(candidate, story, filename):\n",
    "    df = pd.DataFrame({'candidate': [candidate], 'story': [story]})\n",
    "    if not os.path.exists(filename):\n",
    "        df.to_csv(filename, index=False, mode='w')\n",
    "    else:\n",
    "        df.to_csv(filename, index=False, mode='a', header=False)\n",
    "    print(f\"Saved story for candidate: {candidate} to {filename}\")\n",
    "\n",
    "def setup_logging(run_name):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"../logs/{run_name}_story_generation_{timestamp}.log\"\n",
    "    os.makedirs(os.path.dirname(log_filename), exist_ok=True)\n",
    "    logger = logging.getLogger(run_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "# Read results\n",
    "candidates_file = f'../data/{run_name}_candidates.csv'\n",
    "if os.path.exists(candidates_file):\n",
    "    df_candidates = pd.read_csv(candidates_file)\n",
    "    print(f\"Loaded {len(df_candidates)} candidates from {candidates_file}\")\n",
    "else:\n",
    "    print(f\"Error: Candidates file {candidates_file} not found.\")\n",
    "    df_candidates = pd.DataFrame(columns=['phrase'])\n",
    "# Build list of tuples (candidate, prompt)\n",
    "prompts = [\n",
    "    (candidate, f\"{prompt_prefix}{candidate}{prompt_suffix}\")\n",
    "    for candidate in df_candidates['phrase']\n",
    "]\n",
    "\n",
    "async def main():\n",
    "    logger = setup_logging(run_name)\n",
    "    api = NovelAIAPI()\n",
    "    await nai_login(api, auth_method, auth)\n",
    "    total_generations = 0\n",
    "    unsuccessful_attempts = 0\n",
    "    stories_filename = f'../data/{run_name}_stories.csv'\n",
    "    for candidate, prompt in prompts:\n",
    "        for story_num in range(stories_per_candidate_goal):\n",
    "            while True:\n",
    "                try:\n",
    "                    total_generations += 1\n",
    "                    print(f\"Generating story {story_num + 1}/{stories_per_candidate_goal} for candidate: {candidate}\")\n",
    "                    logger.info(f\"Generating story {story_num + 1}/{stories_per_candidate_goal} for candidate: {candidate}\")\n",
    "                    story = await asyncio.wait_for(\n",
    "                        generate_full_story(api, prompt, model, preset, story_words, verbose=True),\n",
    "                        timeout=generation_timeout\n",
    "                    )\n",
    "                    save_story_to_csv(candidate, story, stories_filename)\n",
    "                    logger.info(f\"Saved story for candidate: {candidate}\")\n",
    "                    unsuccessful_attempts = 0\n",
    "                    break\n",
    "                except asyncio.TimeoutError:\n",
    "                    print(\"Generation took too long. Retrying...\")\n",
    "                    logger.warning(\"Generation took too long. Retrying...\")\n",
    "                    unsuccessful_attempts += 1\n",
    "                    if unsuccessful_attempts >= max_failed_gens:\n",
    "                        print(f\"{max_failed_gens} unsuccessful generation attempts. Skipping this story.\")\n",
    "                        logger.error(f\"{max_failed_gens} unsuccessful generation attempts. Skipping this story.\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    logger.error(f\"Error: {e}\")\n",
    "                    unsuccessful_attempts += 1\n",
    "                    if unsuccessful_attempts >= max_failed_gens:\n",
    "                        print(f\"{max_failed_gens} unsuccessful generation attempts. Skipping this story.\")\n",
    "                        logger.error(f\"{max_failed_gens} unsuccessful generation attempts. Skipping this story.\")\n",
    "                        break\n",
    "            time.sleep(delay_time)\n",
    "    print(\"Story generation complete!\")\n",
    "    logger.info(\"Story generation complete!\")\n",
    "\n",
    "# Run the main function\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
