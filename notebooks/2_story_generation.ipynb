{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Story Generation\n",
    "# Imports\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from novelai_api.NovelAI_API import NovelAIAPI\n",
    "from novelai_api.Preset import Model, Preset\n",
    "from novelai_api.GlobalSettings import GlobalSettings\n",
    "from novelai_api.Tokenizer import Tokenizer\n",
    "from novelai_api.utils import b64_to_tokens, tokens_to_b64\n",
    "\n",
    "import IPython.display as display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def parse_config_value(value):\n",
    "    \"\"\"Parse a config value, preserving spaces if it's a quoted string.\"\"\"\n",
    "    value = value.strip()\n",
    "    if (value.startswith(\"'\") and value.endswith(\"'\")) or \\\n",
    "       (value.startswith('\"') and value.endswith('\"')):\n",
    "        return ast.literal_eval(value)\n",
    "    return value\n",
    "\n",
    "# Read Settings\n",
    "config_file = '../config/genre_clio_settings.ini'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "# Access the General Settings\n",
    "run_name = config['GENERAL']['run_name']\n",
    "auth_method = config['GENERAL']['auth_method']\n",
    "\n",
    "# Access Story Generation Settings\n",
    "delay_time = int(config['STORY GENERATION - GEN SETTINGS']['delay_time'])\n",
    "generation_timeout = int(config['STORY GENERATION - GEN SETTINGS']['generation_timeout'])\n",
    "max_failed_gens = int(config['STORY GENERATION - GEN SETTINGS']['max_failed_gens'])\n",
    "stories_per_candidate_goal = int(config['STORY GENERATION - GEN SETTINGS']['stories_per_candidate_goal'])\n",
    "story_words = int(config['STORY GENERATION - GEN SETTINGS']['story_words'])\n",
    "bias_phrases = ast.literal_eval(config['STORY GENERATION - GEN SETTINGS']['bias_phrases'])\n",
    "model_class, model_attr = config['STORY GENERATION - GEN SETTINGS']['model'].split('.')\n",
    "model = getattr(globals()[model_class], model_attr)\n",
    "max_context = int(config['STORY GENERATION - GEN SETTINGS']['max_context'])\n",
    "max_gen_length = int(config['STORY GENERATION - GEN SETTINGS']['max_gen_length'])\n",
    "prompt_prefix = parse_config_value(config['STORY GENERATION - GEN SETTINGS']['prompt_prefix'])\n",
    "prompt_suffix = parse_config_value(config['STORY GENERATION - GEN SETTINGS']['prompt_suffix'])\n",
    "\n",
    "# Access the Preset Configuration\n",
    "preset_method = config['STORY GENERATION - PRESET']['preset_method']\n",
    "preset_name = config['STORY GENERATION - PRESET']['preset_name']\n",
    "\n",
    "if preset_method == \"custom\":\n",
    "    preset_stop_sequences = ast.literal_eval(config['STORY GENERATION - PRESET']['preset_stop_sequences'])\n",
    "    preset_temperature = float(config['STORY GENERATION - PRESET']['preset_temperature'])\n",
    "    preset_max_length = int(config['STORY GENERATION - PRESET']['preset_max_length'])\n",
    "    preset_min_length = int(config['STORY GENERATION - PRESET']['preset_min_length'])\n",
    "    preset_top_k = int(config['STORY GENERATION - PRESET']['preset_top_k'])\n",
    "    preset_top_a = float(config['STORY GENERATION - PRESET']['preset_top_a'])\n",
    "    preset_top_p = float(config['STORY GENERATION - PRESET']['preset_top_p'])\n",
    "    preset_typical_p = float(config['STORY GENERATION - PRESET']['preset_typical_p'])\n",
    "    preset_tail_free_sampling = float(config['STORY GENERATION - PRESET']['preset_tail_free_sampling'])\n",
    "    preset_repetition_penalty = float(config['STORY GENERATION - PRESET']['preset_repetition_penalty'])\n",
    "    preset_repetition_penalty_range = int(config['STORY GENERATION - PRESET']['preset_repetition_penalty_range'])\n",
    "    preset_repetition_penalty_slope = float(config['STORY GENERATION - PRESET']['preset_repetition_penalty_slope'])\n",
    "    preset_repetition_penalty_frequency = float(config['STORY GENERATION - PRESET']['preset_repetition_penalty_frequency'])\n",
    "    preset_repetition_penalty_presence = float(config['STORY GENERATION - PRESET']['preset_repetition_penalty_presence'])\n",
    "    preset_repetition_penalty_whitelist = ast.literal_eval(config['STORY GENERATION - PRESET']['preset_repetition_penalty_whitelist'])\n",
    "    preset_repetition_penalty_default_whitelist = config['STORY GENERATION - PRESET']['preset_repetition_penalty_default_whitelist'] == 'True'\n",
    "    preset_length_penalty = float(config['STORY GENERATION - PRESET']['preset_length_penalty'])\n",
    "    preset_diversity_penalty = float(config['STORY GENERATION - PRESET']['preset_diversity_penalty'])\n",
    "    preset_order = ast.literal_eval(config['STORY GENERATION - PRESET']['preset_order'])\n",
    "    preset_phrase_rep_pen = config['STORY GENERATION - PRESET']['preset_phrase_rep_pen']\n",
    "\n",
    "    preset = Preset(name=preset_name, model=model, settings={\n",
    "        'temperature': preset_temperature,\n",
    "        'max_length': preset_max_length,\n",
    "        'min_length': preset_min_length,\n",
    "        'top_k': preset_top_k,\n",
    "        'top_a': preset_top_a,\n",
    "        'top_p': preset_top_p,\n",
    "        'typical_p': preset_typical_p,\n",
    "        'tail_free_sampling': preset_tail_free_sampling,\n",
    "        'repetition_penalty': preset_repetition_penalty,\n",
    "        'repetition_penalty_range': preset_repetition_penalty_range,\n",
    "        'repetition_penalty_slope': preset_repetition_penalty_slope,\n",
    "        'repetition_penalty_frequency': preset_repetition_penalty_frequency,\n",
    "        'repetition_penalty_presence': preset_repetition_penalty_presence,\n",
    "        'repetition_penalty_whitelist': preset_repetition_penalty_whitelist,\n",
    "        'repetition_penalty_default_whitelist': preset_repetition_penalty_default_whitelist,\n",
    "        'length_penalty': preset_length_penalty,\n",
    "        'diversity_penalty': preset_diversity_penalty,\n",
    "        'order': preset_order,\n",
    "        'phrase_rep_pen': preset_phrase_rep_pen,\n",
    "    })\n",
    "\n",
    "elif preset_method == \"official\":\n",
    "    preset = preset_name  # We'll use this string to get the official preset in gen_story\n",
    "else:\n",
    "    raise ValueError(f\"Invalid preset_method: {preset_method}. Must be 'custom' or 'official'.\")\n",
    "\n",
    "auth = False\n",
    "env = os.environ\n",
    "\n",
    "# Init variable for login method\n",
    "if auth_method == \"enter_key\":\n",
    "    auth = input(\"Enter your NovelAI access key: \")\n",
    "if auth_method == \"enter_token\":\n",
    "    auth = input(\"Enter your NovelAI access token: \")\n",
    "elif auth_method == \"enter_login\":\n",
    "    auth = {}\n",
    "    auth[\"user\"] = input(\"Enter your NovelAI username: \")\n",
    "    auth[\"pw\"] = input(\"Enter your NovelAI password: \")\n",
    "elif auth_method == \"env_key\":\n",
    "    auth = env[\"NAI_KEY\"]\n",
    "elif auth_method == \"env_token\":\n",
    "    auth = env[\"NAI_TOKEN\"]\n",
    "elif auth_method == \"env_login\":\n",
    "    auth = {}\n",
    "    auth[\"user\"] = env[\"NAI_USERNAME\"]\n",
    "    auth[\"pw\"] = env[\"NAI_PASSWORD\"]\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"Invalid value for 'auth_method'. Must be one of 'enter_key', 'enter_token', 'enter_login', 'env_key', 'env_token' or 'env_login\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "async def nai_login(api, auth_method, auth):\n",
    "    if auth_method == \"enter_key\" or auth_method == \"env_key\":\n",
    "        await api.high_level.login_from_key(auth)\n",
    "    elif auth_method == \"enter_token\" or auth_method == \"env_token\":\n",
    "        await api.high_level.login_with_token(auth)\n",
    "    elif auth_method == \"enter_login\" or auth_method == \"env_login\":\n",
    "        await api.high_level.login(auth[\"user\"], auth[\"pw\"])\n",
    "\n",
    "async def gen_story(api, prompt, model, preset, max_length=max_gen_length):\n",
    "    global_settings = GlobalSettings()\n",
    "    if isinstance(preset, str):\n",
    "        preset = Preset.from_official(model, preset)\n",
    "    preset['max_length'] = min(max_length, max_gen_length)\n",
    "    gen = await api.high_level.generate(\n",
    "        prompt, model, preset, global_settings, None, None, None\n",
    "    )\n",
    "    generated_text = Tokenizer.decode(model, b64_to_tokens(gen[\"output\"]))\n",
    "    return generated_text\n",
    "\n",
    "def text_to_html(text):\n",
    "    \"\"\"Convert plain text to HTML, preserving line breaks.\"\"\"\n",
    "    return text.replace('\\n', '<br>')\n",
    "\n",
    "async def generate_full_story(api, prompt, model, preset, story_words, story_display, max_retries=3, verbose=False, logger=None):\n",
    "    full_story = prompt\n",
    "    current_prompt = prompt\n",
    "    buffer_tokens = 500  # Buffer to prevent constantly hitting max_context\n",
    "    \n",
    "    if verbose:\n",
    "        logger.info(\"*\"*50)\n",
    "        logger.info(f\"Initial max_context: {max_context}\")\n",
    "        logger.info(f\"Initial prompt length: {len(prompt.split())} words\")\n",
    "    \n",
    "    while len(full_story.split()) < story_words:\n",
    "        remaining_words = story_words - len(full_story.split())\n",
    "        prompt_tokens = Tokenizer.encode(model, current_prompt)\n",
    "        \n",
    "        if verbose:\n",
    "            logger.info(\"*\"*50)\n",
    "            logger.info(f\"Current prompt tokens: {len(prompt_tokens)}\")\n",
    "            logger.info(f\"Remaining words: {remaining_words}\")\n",
    "        \n",
    "        available_tokens = max_context - len(prompt_tokens) - buffer_tokens\n",
    "        max_length = min(max(available_tokens, int(remaining_words * 1.33)), max_gen_length)\n",
    "        \n",
    "        if verbose:\n",
    "            logger.info(f\"Available tokens: {available_tokens}\")\n",
    "            logger.info(f\"Max length for generation: {max_length}\")\n",
    "        \n",
    "        if len(prompt_tokens) + max_length > max_context - buffer_tokens:\n",
    "            max_length = max_context - len(prompt_tokens) - buffer_tokens - 1\n",
    "            if verbose:\n",
    "                logger.info(f\"Adjusted max_length to {max_length} to prevent exceeding max_context\")\n",
    "        \n",
    "        if max_length < 1:\n",
    "            logger.info(\"Cannot generate more tokens without exceeding max_context. Trimming context.\")\n",
    "            # Trim the current prompt to make room for new generation\n",
    "            trim_tokens = int(max_context * 0.1)  # Trim 10% of max_context instead of 20%\n",
    "            current_prompt_tokens = prompt_tokens[trim_tokens:]\n",
    "            current_prompt = Tokenizer.decode(model, current_prompt_tokens)\n",
    "            continue\n",
    "        \n",
    "        for _ in range(max_retries):\n",
    "            try:\n",
    "                generated_text = await gen_story(api, current_prompt, model, preset, max_length)\n",
    "                full_story += generated_text\n",
    "                display.update_display(HTML(text_to_html(full_story)), display_id=story_display.display_id)\n",
    "                if verbose:\n",
    "                    logger.info(f\"Generated text length: {len(generated_text.split())} words\")\n",
    "                    logger.info(f\"Full story length: {len(full_story.split())} words\")\n",
    "                \n",
    "                # Update the prompt for the next iteration\n",
    "                full_story_tokens = Tokenizer.encode(model, full_story)\n",
    "                if len(full_story_tokens) > max_context - buffer_tokens:\n",
    "                    # If the full story exceeds max_context - buffer, trim it\n",
    "                    keep_tokens = int(max_context * 0.9)  # Keep 90% of max_context\n",
    "                    trimmed_tokens = full_story_tokens[-keep_tokens:]\n",
    "                    current_prompt = Tokenizer.decode(model, trimmed_tokens)\n",
    "                    if verbose:\n",
    "                        logger.info(\"*\"*50)\n",
    "                        logger.info(\"Trimming context for next generation\")\n",
    "                        logger.info(f\"Trimmed prompt to {len(trimmed_tokens)} tokens\")\n",
    "                else:\n",
    "                    current_prompt = full_story\n",
    "                    if verbose:\n",
    "                        logger.info(\"*\"*50)\n",
    "                        logger.info(\"Using full story as prompt for next generation.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during generation: {e}. Retrying...\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to generate after {max_retries} attempts. Stopping generation.\")\n",
    "            break\n",
    "    \n",
    "    return full_story\n",
    "\n",
    "def save_story_to_csv(candidate, story, filename):\n",
    "    df = pd.DataFrame({'candidate': [candidate], 'story': [story]})\n",
    "    if not os.path.exists(filename):\n",
    "        df.to_csv(filename, index=False, mode='w')\n",
    "    else:\n",
    "        df.to_csv(filename, index=False, mode='a', header=False)\n",
    "    print(f\"Saved story for candidate: {candidate} to {filename}\")\n",
    "\n",
    "def setup_logging(run_name):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"../logs/{run_name}_story_generation_{timestamp}.log\"\n",
    "    os.makedirs(os.path.dirname(log_filename), exist_ok=True)\n",
    "    logger = logging.getLogger(run_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 candidates from ../data/genre_clio_candidates.csv\n"
     ]
    }
   ],
   "source": [
    "# Read results\n",
    "candidates_file = f'../data/{run_name}_candidates.csv'\n",
    "if os.path.exists(candidates_file):\n",
    "    df_candidates = pd.read_csv(candidates_file)\n",
    "    print(f\"Loaded {len(df_candidates)} candidates from {candidates_file}\")\n",
    "else:\n",
    "    print(f\"Error: Candidates file {candidates_file} not found.\")\n",
    "    df_candidates = pd.DataFrame(columns=['phrase'])\n",
    "\n",
    "# Build list of tuples (candidate, prompt)\n",
    "prompts = [\n",
    "    (candidate, f\"{prompt_prefix}{candidate}{prompt_suffix}\")\n",
    "    for candidate in df_candidates['phrase']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating story 1/30 for candidate: Slice of Life\n",
      "Current Full Story:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "[ Genre: Slice of Life ]<br>Summary: A series of letters from the son to the father of the man who was the model for his late grandfather.<br>The author was a prominent novelist in the Heian period. He was well known for his literary work, but he was also well known for the \"writings in the sand\" game that was extremely popular at the time, where people wrote their desires on the beach or on the rocks that formed the banks of rivers and ponds. The author was in love with a girl who would sometimes write wishes on the beach. His \"story\" is a series of letters that were written to her by a man who is never identified. He is simply referred to as \"the father\" in the letters.<br>The man who is the model for \"the father\" was a literary man who was also known for writing his feelings on the shore, which is what gave the man his nickname, \"the Seashore Writer.\" Although it's called a novel, it's more of a personal journal or a love letter.<br>The author used the setting of his grandfather, his daughter's father, as a standin for the man who was the model. This is the tale of the grandson and the father, in which they talk about the past, discuss their feelings, and share their hopes for the future. It's a classic, timeless piece of literature.<br>----<br>Let's Make a Sand Castle<br>Type: manga, short story<br>Release date: 1997<br>Released on the tenth anniversary of the Osaka Castle Shinden.<br>A young boy lives with his mother and father and works as a model for his grandfather in a sand castle building contest. His grandfather is a sand castle builder. As an editor, I had the opportunity to work with the author, Kenji Miyazawa, on this anthology book that celebrated the castle.<br>Kenji Miyazawa lived in a small town in Iwate Prefecture. There is a large sand castle building event in that town every year. His grandson often goes with him to the event, and the two of them talk about sand castles while they walk around the town. Kenji Miyazawa had just published the first of the Night on the Galactic Railroad series, a series that was his most famous work, and he had a habit of talking about it to his grandson while they walked around town.<br>His grandson later wrote a children's book based on the event. When I read it, I realized that Miyazawa's stories weren't just for children. Miyazawa had a gentle personality that made him popular with children and adults alike.<br>----<br>The Tiniest Painting in the World<br>Type: TV special, TV special<br>Release date: 1997<br>Director: Eiichi Yamamoto<br>Writer: Mamoru Kodama<br>Producer: Production IG, Shueisha, TV Asahi<br>A father and son search for the tiniest painting in the world. They visit a man who has a special skill that can help them find it, but when they arrive, they find he is not the man they think he is.<br>It's the work of an artist who won a contest with his painting that is less than one hundredth the size of a grain of sand. The contest is supposed to be an unveiling of his works. However, it's not the unveiling of his works that has garnered attention, but a young boy who comes along with his father. The boy looks like a painter, but when the artist notices he is much older than the time when he entered the contest, he realizes the boy has stolen one of his works and replaced it with an identical-looking painting.<br>The father is a painter, too, and he's devastated to see the thief has stolen his son's work. The father and son go to the police to try to return the work, but when they arrive, they find out the prize has been awarded to a young girl in a white coat who has been working on a different painting, one much larger than his son's.<br>They realize they don't have a clue what the artist has been doing. What has he been creating with the tiny painting? They go to a man who collects paintings, hoping to learn something from him.<br>When they meet the man, they find"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStory generation complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\nai-attg-tester-CNuytz7O-py3.11\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\nai-attg-tester-CNuytz7O-py3.11\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\nai-attg-tester-CNuytz7O-py3.11\\Lib\\site-packages\\nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     heappop(scheduled)\n\u001b[0;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\selectors.py:323\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    321\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\selectors.py:314\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 314\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m select\u001b[38;5;241m.\u001b[39mselect(r, w, w, timeout)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "async def main():\n",
    "    logger = setup_logging(run_name)\n",
    "    api = NovelAIAPI()\n",
    "    await nai_login(api, auth_method, auth)\n",
    "    total_generations = 0\n",
    "    unsuccessful_attempts = 0\n",
    "    stories_filename = f'../data/{run_name}_stories.csv'\n",
    "    \n",
    "    for candidate, prompt in prompts:\n",
    "        for story_num in range(stories_per_candidate_goal):\n",
    "            while True:\n",
    "                try:\n",
    "                    total_generations += 1\n",
    "                    print(f\"Generating story {story_num + 1}/{stories_per_candidate_goal} for candidate: {candidate}\")\n",
    "                    print(\"Current Full Story:\")\n",
    "                    logger.info(f\"Generating story {story_num + 1}/{stories_per_candidate_goal} for candidate: {candidate}\")\n",
    "                    \n",
    "                    story_display = display.display(HTML(text_to_html(prompt)), display_id=True)\n",
    "                    story = await asyncio.wait_for(\n",
    "                        generate_full_story(api, prompt, model, preset, story_words, story_display, verbose=True, logger=logger),\n",
    "                        timeout=generation_timeout\n",
    "                    )\n",
    "                    \n",
    "                    save_story_to_csv(candidate, story, stories_filename)\n",
    "                    logger.info(f\"Saved story for candidate: {candidate}\")\n",
    "                    unsuccessful_attempts = 0\n",
    "                    break\n",
    "                except asyncio.TimeoutError:\n",
    "                    print(\"Generation took too long. Retrying...\")\n",
    "                    logger.warning(\"Generation took too long. Retrying...\")\n",
    "                    unsuccessful_attempts += 1\n",
    "                    if unsuccessful_attempts >= max_failed_gens:\n",
    "                        print(f\"{max_failed_gens} unsuccessful generation attempts. Skipping this story.\")\n",
    "                        logger.error(f\"{max_failed_gens} unsuccessful generation attempts. Skipping this story.\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    logger.error(f\"Error: {e}\")\n",
    "                    unsuccessful_attempts += 1\n",
    "                    if unsuccessful_attempts >= max_failed_gens:\n",
    "                        print(f\"{max_failed_gens} unsuccessful generation attempts. Skipping this story.\")\n",
    "                        logger.error(f\"{max_failed_gens} unsuccessful generation attempts. Skipping this story.\")\n",
    "                        break\n",
    "            time.sleep(delay_time)\n",
    "    print(\"Story generation complete!\")\n",
    "    logger.info(\"Story generation complete!\")\n",
    "\n",
    "# Run the main function\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
