{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import configparser\n",
    "import ast\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from novelai_api.NovelAI_API import NovelAIAPI\n",
    "from novelai_api.Preset import Model, Preset\n",
    "from novelai_api.GlobalSettings import GlobalSettings\n",
    "from novelai_api.Tokenizer import Tokenizer\n",
    "from novelai_api.utils import b64_to_tokens\n",
    "from novelai_api.BiasGroup import BiasGroup\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Settings\n",
    "config_file = '../config/genre_clio_settings.ini'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "# Access the General Settings\n",
    "run_name = config['GENERAL']['run_name']\n",
    "auth_method = config['GENERAL']['auth_method']\n",
    "\n",
    "# Access the Generation Settings\n",
    "delay_time = int(config['GENERATION SETTINGS']['delay_time'])\n",
    "generation_timeout = int(config['GENERATION SETTINGS']['generation_timeout'])\n",
    "max_failed_gens = int(config['GENERATION SETTINGS']['max_failed_gens'])\n",
    "candidates_goal = int(config['GENERATION SETTINGS']['candidates_goal'])\n",
    "bias_strength_inc = float(config['GENERATION SETTINGS']['bias_strength_inc'])\n",
    "bias_phrases = ast.literal_eval(config['GENERATION SETTINGS']['bias_phrases'])\n",
    "model_class, model_attr = config['GENERATION SETTINGS']['model'].split('.')\n",
    "model = getattr(globals()[model_class], model_attr)\n",
    "prompt = config['GENERATION SETTINGS']['prompt']\n",
    "stop_sequences = ast.literal_eval(config['GENERATION SETTINGS']['stop_sequences'])\n",
    "checkpoint_interval = int(config['GENERATION SETTINGS']['checkpoint_interval'])\n",
    "\n",
    "# Access the Preset Configuration\n",
    "preset_method = config['PRESET']['preset_method']\n",
    "preset_name = config['PRESET']['preset_name']\n",
    "preset_stop_sequences = ast.literal_eval(config['PRESET']['preset_stop_sequences'])\n",
    "preset_temperature = float(config['PRESET']['preset_temperature'])\n",
    "preset_max_length = int(config['PRESET']['preset_max_length'])\n",
    "preset_min_length = int(config['PRESET']['preset_min_length'])\n",
    "preset_top_k = int(config['PRESET']['preset_top_k'])\n",
    "preset_top_a = float(config['PRESET']['preset_top_a'])\n",
    "preset_top_p = float(config['PRESET']['preset_top_p'])\n",
    "preset_typical_p = float(config['PRESET']['preset_typical_p'])\n",
    "preset_tail_free_sampling = float(config['PRESET']['preset_tail_free_sampling'])\n",
    "preset_repetition_penalty = float(config['PRESET']['preset_repetition_penalty'])\n",
    "preset_repetition_penalty_range = int(config['PRESET']['preset_repetition_penalty_range'])\n",
    "preset_repetition_penalty_slope = float(config['PRESET']['preset_repetition_penalty_slope'])\n",
    "preset_repetition_penalty_frequency = float(config['PRESET']['preset_repetition_penalty_frequency'])\n",
    "preset_repetition_penalty_presence = float(config['PRESET']['preset_repetition_penalty_presence'])\n",
    "preset_repetition_penalty_whitelist = ast.literal_eval(config['PRESET']['preset_repetition_penalty_whitelist'])\n",
    "preset_repetition_penalty_default_whitelist = config['PRESET']['preset_repetition_penalty_default_whitelist'] == 'True'\n",
    "preset_length_penalty = float(config['PRESET']['preset_length_penalty'])\n",
    "preset_diversity_penalty = float(config['PRESET']['preset_diversity_penalty'])\n",
    "preset_order = ast.literal_eval(config['PRESET']['preset_order'])\n",
    "preset_phrase_rep_pen = config['PRESET']['preset_phrase_rep_pen']\n",
    "\n",
    "# Assuming Preset is a class you have defined or imported\n",
    "preset = Preset(name=preset_name, model=model, settings={\n",
    "    'temperature': preset_temperature,\n",
    "    'max_length': preset_max_length,\n",
    "    'min_length': preset_min_length,\n",
    "    'top_k': preset_top_k,\n",
    "    'top_a': preset_top_a,\n",
    "    'top_p': preset_top_p,\n",
    "    'typical_p': preset_typical_p,\n",
    "    'tail_free_sampling': preset_tail_free_sampling,\n",
    "    'repetition_penalty': preset_repetition_penalty,\n",
    "    'repetition_penalty_range': preset_repetition_penalty_range,\n",
    "    'repetition_penalty_slope': preset_repetition_penalty_slope,\n",
    "    'repetition_penalty_frequency': preset_repetition_penalty_frequency,\n",
    "    'repetition_penalty_presence': preset_repetition_penalty_presence,\n",
    "    'repetition_penalty_whitelist': preset_repetition_penalty_whitelist,\n",
    "    'repetition_penalty_default_whitelist': preset_repetition_penalty_default_whitelist,\n",
    "    'length_penalty': preset_length_penalty,\n",
    "    'diversity_penalty': preset_diversity_penalty,\n",
    "    'order': preset_order,\n",
    "    'phrase_rep_pen': preset_phrase_rep_pen,\n",
    "})\n",
    "\n",
    "auth = False\n",
    "env = os.environ\n",
    "\n",
    "# Init variable for login method\n",
    "if auth_method == \"enter_key\":\n",
    "    auth = input(\"Enter your NovelAI access key: \")\n",
    "if auth_method == \"enter_token\":\n",
    "    auth = input(\"Enter your NovelAI access token: \")\n",
    "elif auth_method == \"enter_login\":\n",
    "    auth = {}\n",
    "    auth[\"user\"] = input(\"Enter your NovelAI username: \")\n",
    "    auth[\"pw\"] = input(\"Enter your NovelAI password: \")\n",
    "elif auth_method == \"env_key\":\n",
    "    auth = env[\"NAI_KEY\"]\n",
    "elif auth_method == \"env_token\":\n",
    "    auth = env[\"NAI_TOKEN\"]\n",
    "elif auth_method == \"env_login\":\n",
    "    auth = {}\n",
    "    auth[\"user\"] = env[\"NAI_USERNAME\"]\n",
    "    auth[\"pw\"] = env[\"NAI_PASSWORD\"]\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"Invalid value for 'auth_method'. Must be one of 'enter_key', 'enter_token', 'enter_login', env_key', 'env_token' or 'env_login\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "\n",
    "async def nai_login(api, auth_method, auth):\n",
    "    if auth_method == \"enter_key\" or auth_method == \"env_key\":\n",
    "        await api.high_level.login_from_key(auth)\n",
    "    elif auth_method == \"enter_token\" or auth_method == \"env_token\":\n",
    "        await api.high_level.login_with_token(auth)\n",
    "    elif auth_method == \"enter_login\" or auth_method == \"env_login\":\n",
    "        await api.high_level.login(auth[\"user\"], auth[\"pw\"])\n",
    "\n",
    "async def gen_attg_candidate(\n",
    "    model=Model.Clio,\n",
    "    preset = \"Edgewise\",\n",
    "    prompt=\"[ Genre:\",\n",
    "    stop_sequences=[\",\", \";\", \" ]\",\"\\n\"],\n",
    "    cut_stop_seq=True,\n",
    "    auth_method=\"env_token\",\n",
    "    auth=None,\n",
    "    bias_groups=None,\n",
    "):\n",
    "    # Initialize the NovelAI API\n",
    "    api = NovelAIAPI()\n",
    "\n",
    "    try:\n",
    "        # Ensure you're logged in\n",
    "        await nai_login(api, auth_method, auth)\n",
    "\n",
    "        # If preset is a string, get the official preset with that name for the specified model\n",
    "        if isinstance(preset, str):\n",
    "            preset = Preset.from_official(model, preset)\n",
    "\n",
    "        # Tokenize the stop sequences and set them for the preset\n",
    "        stop_sequences_tokenized = [\n",
    "            Tokenizer.encode(model, seq) for seq in stop_sequences\n",
    "        ]\n",
    "        preset[\"stop_sequences\"] = stop_sequences_tokenized\n",
    "\n",
    "        # Create default global settings\n",
    "        global_settings = GlobalSettings()\n",
    "\n",
    "        gen = await api.high_level.generate(\n",
    "            prompt, model, preset, global_settings, None, bias_groups, None\n",
    "        )\n",
    "\n",
    "        # After generating the text, remove the stop sequence\n",
    "        generated_text = Tokenizer.decode(model, b64_to_tokens(gen[\"output\"]))\n",
    "        if cut_stop_seq:\n",
    "            for seq in stop_sequences:\n",
    "                generated_text = re.sub(\n",
    "                    re.escape(seq) + \"$\", \"\", generated_text\n",
    "                ).strip()\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error generating text: {e}\")\n",
    "\n",
    "def update_bias_groups(phrase, bias_phrase_dict, bias_strength_inc, bias_groups):\n",
    "    # Update the bias strength for the phrase or add it if it's not in the dict\n",
    "    if phrase in bias_phrase_dict:\n",
    "        bias_phrase_dict[phrase] += bias_strength_inc\n",
    "    else:\n",
    "        bias_phrase_dict[phrase] = bias_strength_inc\n",
    "\n",
    "    # Clear the existing bias groups\n",
    "    bias_groups.clear()\n",
    "\n",
    "    # Regenerate the bias groups based on the updated bias_phrase_dict\n",
    "    for phrase, strength in bias_phrases.items():\n",
    "        bg = BiasGroup(strength)\n",
    "        bg.add(phrase)\n",
    "        bias_groups.append(bg)\n",
    "\n",
    "def load_existing_results(run_name):\n",
    "    filename = f\"../data/{run_name}_results.csv\"\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        bias_phrases = dict(zip(df['phrase'], df['last_bias']))\n",
    "        return df, bias_phrases\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"phrase\", \"count\", \"last_bias\"]), {}\n",
    "\n",
    "def update_run_info(run_name, settings, terms_generated, terms_added, status, start_time, is_checkpoint=False):\n",
    "    filename = f\"../data/{run_name}_run_info.csv\"\n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    run_info = pd.DataFrame({\n",
    "        'timestamp': [end_time],\n",
    "        'terms_generated': [terms_generated],\n",
    "        'terms_added': [terms_added],\n",
    "        'status': [status],\n",
    "        'duration': [duration],\n",
    "        'is_checkpoint': [is_checkpoint],\n",
    "        **settings\n",
    "    })\n",
    "    if os.path.exists(filename):\n",
    "        existing_info = pd.read_csv(filename)\n",
    "        # Remove previous checkpoints if this is a new checkpoint\n",
    "        if is_checkpoint:\n",
    "            existing_info = existing_info[existing_info['is_checkpoint'] == False]\n",
    "        updated_info = pd.concat([existing_info, run_info], ignore_index=True)\n",
    "    else:\n",
    "        updated_info = run_info\n",
    "    updated_info.to_csv(filename, index=False)\n",
    "\n",
    "class ImmediateFileHandler(logging.FileHandler):\n",
    "    def emit(self, record):\n",
    "        super().emit(record)\n",
    "        self.flush()\n",
    "\n",
    "def setup_logging(run_name):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"../logs/{run_name}_{timestamp}.log\"\n",
    "    os.makedirs(os.path.dirname(log_filename), exist_ok=True)\n",
    "    \n",
    "    logger = logging.getLogger(run_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    file_handler = ImmediateFileHandler(log_filename)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "async def main():\n",
    "    # Setup logging\n",
    "    logger = setup_logging(run_name)\n",
    "\n",
    "    # Load existing results if available\n",
    "    df, bias_phrases = load_existing_results(run_name)\n",
    "\n",
    "    # Initialize bias groups\n",
    "    bias_groups = []\n",
    "    for phrase, strength in bias_phrases.items():\n",
    "        bg = BiasGroup(strength)\n",
    "        bg.add(phrase)\n",
    "        bias_groups.append(bg)\n",
    "\n",
    "    # Counter for total generations and unsuccessful attempts\n",
    "    total_generations = 0\n",
    "    unsuccessful_attempts = 0\n",
    "    terms_added = 0\n",
    "\n",
    "    # Initialize settings_data outside the loop\n",
    "    settings_data = {\n",
    "        \"auth_method\": auth_method,\n",
    "        \"candidates_goal\": candidates_goal,\n",
    "        \"bias_strength_inc\": bias_strength_inc,\n",
    "        \"model\": str(model),\n",
    "        \"preset_name\": preset.name,\n",
    "        \"preset_settings\": str(preset._settings),\n",
    "        \"bias_phrases\": str(bias_phrases),\n",
    "        \"checkpoint_interval\": checkpoint_interval\n",
    "    }\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Loop until you have candidate_goal unique phrases\n",
    "    try:\n",
    "        while len(df) < candidates_goal:\n",
    "            total_generations += 1\n",
    "\n",
    "            # Clear the previous output\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            print(f\"Gen {total_generations}: Trying to gen phrase {len(df)+1}/{candidates_goal}...\")\n",
    "            logger.info(f\"Gen {total_generations}: Trying to gen phrase {len(df)+1}/{candidates_goal}...\")\n",
    "\n",
    "            try:\n",
    "                phrase = await asyncio.wait_for(\n",
    "                    gen_attg_candidate(\n",
    "                        model=model,\n",
    "                        preset=preset,\n",
    "                        prompt=prompt,\n",
    "                        auth_method=auth_method,\n",
    "                        auth=auth,\n",
    "                        bias_groups=bias_groups,\n",
    "                    ),\n",
    "                    timeout=generation_timeout,\n",
    "                )\n",
    "\n",
    "                # Check if the phrase is already in the DataFrame\n",
    "                if phrase in df[\"phrase\"].values:\n",
    "                    df.loc[df[\"phrase\"] == phrase, \"count\"] += 1\n",
    "                    df.loc[df[\"phrase\"] == phrase, \"last_bias\"] = bias_phrases.get(phrase, 0)\n",
    "                    print(f\"Phrase '{phrase}' already exists. Incrementing count and changing bias by {bias_strength_inc}.\")\n",
    "                    logger.info(f\"Phrase '{phrase}' already exists. Incrementing count and changing bias by {bias_strength_inc}.\")\n",
    "\n",
    "                    # Update the bias groups since the phrase was generated again\n",
    "                    update_bias_groups(phrase, bias_phrases, bias_strength_inc, bias_groups)\n",
    "                else:\n",
    "                    df.loc[len(df)] = [phrase, 1, bias_phrases.get(phrase, 0)]\n",
    "                    print(f\"Added new phrase: '{phrase}'\")\n",
    "                    logger.info(f\"Added new phrase: '{phrase}'\")\n",
    "                    terms_added += 1\n",
    "\n",
    "                # Reset the unsuccessful_attempts counter if generation was successful\n",
    "                unsuccessful_attempts = 0\n",
    "\n",
    "                # Store results and settings\n",
    "                filename_results = f\"../data/{run_name}_results.csv\"\n",
    "                df.to_csv(filename_results, index=False)\n",
    "\n",
    "                # Update settings_data with the latest bias_phrases\n",
    "                settings_data[\"bias_phrases\"] = str(bias_phrases)\n",
    "\n",
    "                print(f\"Saved progress to {filename_results}.\")\n",
    "                logger.info(f\"Saved progress to {filename_results}.\")\n",
    "\n",
    "                # Save checkpoint every checkpoint_interval generations\n",
    "                if total_generations % checkpoint_interval == 0:\n",
    "                    update_run_info(run_name, settings_data, total_generations, terms_added, \"ongoing\", start_time, is_checkpoint=True)\n",
    "                    print(f\"Checkpoint saved at {total_generations} generations.\")\n",
    "                    logger.info(f\"Checkpoint saved at {total_generations} generations.\")\n",
    "\n",
    "            except asyncio.TimeoutError:\n",
    "                print(\"Generation took too long. Retrying...\")\n",
    "                logger.warning(\"Generation took too long. Retrying...\")\n",
    "                unsuccessful_attempts += 1\n",
    "                if unsuccessful_attempts >= max_failed_gens:\n",
    "                    print(f\"{max_failed_gens} unsuccessful generation attempts. Aborting candidate search.\")\n",
    "                    logger.error(f\"{max_failed_gens} unsuccessful generation attempts. Aborting candidate search.\")\n",
    "                    update_run_info(run_name, settings_data, total_generations, terms_added, \"aborted\", start_time)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                if \"Anonymous quota reached\" in str(e):\n",
    "                    print(f\"Error: {e}\")\n",
    "                    print(\"Anonymous rate limit reached. This indicates you are not properly authenticated. Check your authentication method. Aborting candidate search.\")\n",
    "                    logger.error(f\"Error: {e}\")\n",
    "                    logger.error(\"Anonymous rate limit reached. This indicates you are not properly authenticated. Check your authentication method. Aborting candidate search.\")\n",
    "                    update_run_info(run_name, settings_data, total_generations, terms_added, \"aborted\", start_time)\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    print(\"Aborting candidate search\")\n",
    "                    logger.error(f\"Error: {e}\")\n",
    "                    logger.error(\"Aborting candidate search\")\n",
    "                    update_run_info(run_name, settings_data, total_generations, terms_added, \"aborted\", start_time)\n",
    "                    break\n",
    "\n",
    "            # Wait for delay_time seconds before the next generation attempt\n",
    "            time.sleep(delay_time)\n",
    "\n",
    "        # Final update of run info if completed successfully\n",
    "        if len(df) >= candidates_goal:\n",
    "            update_run_info(run_name, settings_data, total_generations, terms_added, \"completed\", start_time)\n",
    "\n",
    "        print(\"\\nCandidate search complete!\")\n",
    "        print(\"Top 10 terms:\")\n",
    "        print(df.sort_values(by=\"count\", ascending=False).head(10))\n",
    "        logger.info(\"\\nCandidate search complete!\")\n",
    "        logger.info(\"Top 10 terms:\")\n",
    "        logger.info(df.sort_values(by=\"count\", ascending=False).head(10))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nRun interrupted by user.\")\n",
    "        logger.info(\"Run interrupted by user.\")\n",
    "        update_run_info(run_name, settings_data, total_generations, terms_added, \"interrupted\", start_time)\n",
    "\n",
    "# Run the main function\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
